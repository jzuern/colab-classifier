{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ROpdMiqMgI-0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prerequesites"
      ]
    },
    {
      "metadata": {
        "id": "scafAL9gf2o1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install kaggle"
      ]
    },
    {
      "metadata": {
        "id": "glEUWt1kf1NF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "9f1155f1-0ceb-4103-81c9-3be1005e2dba"
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install tensorflow-plot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/9b/ac57e15fbb239c6793c8d0b7dfd1a4c4a025eaa9f791b5388a7afb515aed/kaggle-1.5.0.tar.gz (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.10.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 7.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/21/3b/a0076243c6ae12a6215b2da515fe06b539aee7217b406e510e\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.5.0 python-slugify-1.2.6\n",
            "Collecting tensorflow-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/41/69/350b59d106813db9ac7591db18b3eedea94894fe732bf897bd9aca74b0b1/tensorflow-plot-0.2.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-plot) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-plot) (1.14.6)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-plot) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->tensorflow-plot) (2.5.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->tensorflow-plot) (2018.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->tensorflow-plot) (2.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->tensorflow-plot) (0.10.0)\n",
            "Building wheels for collected packages: tensorflow-plot\n",
            "  Running setup.py bdist_wheel for tensorflow-plot ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/aa/0d/52/6e050bed5bfe6e39964a4a6e3ba8a0c0e64d37f42ac7af1a75\n",
            "Successfully built tensorflow-plot\n",
            "Installing collected packages: tensorflow-plot\n",
            "Successfully installed tensorflow-plot-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRLJq4Mqf4qx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will be prompted to Authorize Google Cloud SDK to access the content of your Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "GanrvKm6gJ8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5618634-6a2e-4af2-cfce-8b78d3da238c"
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zoiyEjygXR6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the HAM10000 dataset using the kaggle API"
      ]
    },
    {
      "metadata": {
        "id": "Zj2KW_AugYG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e570c2f5-e3a9-4e35-e673-c24a9b667504"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download kmader/skin-cancer-mnist-ham10000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 2.60G/2.62G [00:27<00:00, 95.4MB/s]\n",
            "100% 2.62G/2.62G [00:28<00:00, 100MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F65NoJpCggfM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time to unzip the dataset und to remove the  zip-files after unzipping."
      ]
    },
    {
      "metadata": {
        "id": "uKa7yP9cghUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "d63e8026-7393-406d-d11f-9283fbe85e22"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/dataset_skin\n",
        "!unzip -n skin-cancer-mnist-ham10000.zip -d /content/dataset_skin\n",
        "!unzip -n -q /content/dataset_skin/HAM10000_images_part_1.zip -d /content/dataset_skin/images\n",
        "!unzip -n -q /content/dataset_skin/HAM10000_images_part_2.zip -d /content/dataset_skin/images\n",
        "!rm /content/dataset_skin/HAM10000_images_part_*.zip\n",
        "!rm skin-cancer-mnist-ham10000.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  skin-cancer-mnist-ham10000.zip\n",
            "  inflating: /content/dataset_skin/hmnist_28_28_RGB.csv  \n",
            "  inflating: /content/dataset_skin/HAM10000_metadata.csv  \n",
            "  inflating: /content/dataset_skin/HAM10000_images_part_1.zip  \n",
            "  inflating: /content/dataset_skin/hmnist_28_28_L.csv  \n",
            "  inflating: /content/dataset_skin/hmnist_8_8_L.csv  \n",
            "  inflating: /content/dataset_skin/HAM10000_images_part_2.zip  \n",
            "  inflating: /content/dataset_skin/hmnist_8_8_RGB.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hI6G54Y5g4xe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we make the necessary steps to mount the Google Drive directory in our Python environment by using the `google-drive-ocamlfuse` API.\n",
        "\n",
        "Once again, you will be asked again for permission to allow Google Cloud SDK to access your Google Drive content."
      ]
    },
    {
      "metadata": {
        "id": "x_07TIp1gh1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "08ecb671-c1fe-4af5-b89f-2d9179e76e22"
      },
      "cell_type": "code",
      "source": [
        "# installing stuff\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Authenticate\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "39qshSlHS1Mx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we proceed by cloning my classifier repo from github.com"
      ]
    },
    {
      "metadata": {
        "id": "l_S2kasJTIKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/jzuern/traffic-light-recognition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxN7LXP-_9st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Monitor training using TensorBoard\n",
        "\n",
        "Many thanks to [this stackoverflow answer](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab#48468512)\n",
        "\n",
        "\n",
        "First, let's execute TensorBoard in the environment specifying host and port"
      ]
    },
    {
      "metadata": {
        "id": "R-M5MtRhcRSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "520aec1f-c524-4c4d-bba8-f836833ec6d1"
      },
      "cell_type": "code",
      "source": [
        "# First, download and install ngrok:\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-29 21:13:21--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.0.94.50, 52.203.53.176, 52.201.75.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.0.94.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  9.62MB/s    in 0.5s    \n",
            "\n",
            "2018-10-29 21:13:21 (9.62 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PZezfJtP_mCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a3163eb-3b42-411c-e44c-63c96aa6ee34"
      },
      "cell_type": "code",
      "source": [
        "# #kill all running ngrok instances\n",
        "!pkill -f ngrok\n",
        "\n",
        "# Execute tensorboard\n",
        "LOG_DIR = '/content/drive/classifier/checkpoints/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "# execute ngrok\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Do the tunneling\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://592d5fa8.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XpdN5diwXKM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5055d20-a732-48c6-9d8b-d632e963cbbf"
      },
      "cell_type": "code",
      "source": [
        "!rm /content/dataset_skin/*.tfrecords\n",
        "!rm -rf /content/drive/classifier/checkpoints/*"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/classifier/checkpoints/eval': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uAADbTWTcRUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Start the classifier!**\n",
        "\n",
        "Now we mount our Google Drive directory to `/content/drive`\n",
        "\n",
        "**Important note**: The directory has to be remounted every time Google Drive content changes! --> Every time your code changes"
      ]
    },
    {
      "metadata": {
        "id": "S6lt2KU3g9ZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse -o nonempty drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUkZwM4_HKZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "outputId": "edccf041-6050-4302-8584-c1f57b141228"
      },
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/classifier/main.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serializing sample 0/10016\n",
            "Serializing sample 100/10016\n",
            "Serializing sample 200/10016\n",
            "Serializing sample 300/10016\n",
            "Serializing sample 400/10016\n",
            "Serializing sample 500/10016\n",
            "Serializing sample 600/10016\n",
            "Serializing sample 700/10016\n",
            "Serializing sample 800/10016\n",
            "Serializing sample 900/10016\n",
            "Serializing sample 1000/10016\n",
            "Serializing sample 1100/10016\n",
            "Serializing sample 1200/10016\n",
            "Serializing sample 1300/10016\n",
            "Serializing sample 1400/10016\n",
            "Serializing sample 1500/10016\n",
            "Serializing sample 1600/10016\n",
            "Serializing sample 1700/10016\n",
            "Serializing sample 1800/10016\n",
            "Serializing sample 1900/10016\n",
            "Serializing sample 2000/10016\n",
            "Serializing sample 2100/10016\n",
            "Serializing sample 2200/10016\n",
            "Serializing sample 2300/10016\n",
            "Serializing sample 2400/10016\n",
            "Serializing sample 2500/10016\n",
            "Serializing sample 2600/10016\n",
            "Serializing sample 2700/10016\n",
            "Serializing sample 2800/10016\n",
            "Serializing sample 2900/10016\n",
            "Serializing sample 3000/10016\n",
            "Serializing sample 3100/10016\n",
            "Serializing sample 3200/10016\n",
            "Serializing sample 3300/10016\n",
            "Serializing sample 3400/10016\n",
            "Serializing sample 3500/10016\n",
            "Serializing sample 3600/10016\n",
            "Serializing sample 3700/10016\n",
            "Serializing sample 3800/10016\n",
            "Serializing sample 3900/10016\n",
            "Serializing sample 4000/10016\n",
            "Serializing sample 4100/10016\n",
            "Serializing sample 4200/10016\n",
            "Serializing sample 4300/10016\n",
            "Serializing sample 4400/10016\n",
            "Serializing sample 4500/10016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxcyHoeR3tqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ablation study\n",
        "\n",
        "1.   Number of residual blocks\n",
        "2.   Type of activation function\n",
        "\n"
      ]
    }
  ]
}